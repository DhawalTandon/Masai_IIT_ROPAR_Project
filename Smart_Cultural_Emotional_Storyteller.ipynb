{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Emotional Branching Storyteller\n",
        "### AI Applications â€“ Individual Open Project\n",
        "\n",
        "**Primary Artifact:** Jupyter Notebook (.ipynb)\n",
        "\n",
        "---\n",
        "### Module E: AI Applications â€“ Individual Open Project\n",
        "\n",
        "**Author:** DHAWAL TANDON  \n",
        "**Institution / Course:** Indian Institute of Technology Ropar  \n",
        "**Primary Artifact:** Jupyter Notebook (.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "\n",
        "The objective of this project is to design and implement an **emotion-aware AI storytelling system**\n",
        "that generates **multimedia narratives** based on emotional choices.\n",
        "\n",
        "The entire system â€” including data definition, model logic, generation pipeline,\n",
        "and output visualization â€” is implemented and demonstrated **within this Jupyter Notebook**.\n",
        "\n",
        "Rather than focusing on spectacle, the system prioritizes:\n",
        "\n",
        "- emotional tone\n",
        "- narrative intimacy\n",
        "- choice-driven meaning\n",
        "- slow, reflective visual storytelling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rx3UUbFo5y_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Problem Definition & Objective\n",
        "\n",
        "### a. Selected Project Track\n",
        "This project is developed under the **AI Applications â€“ Open Project (Generative AI / NLP)** track.\n",
        "\n",
        "### b. Problem Statement\n",
        "Most existing AI storytelling systems focus on logical or factual coherence,\n",
        "but fail to adapt narratives based on **emotional states**.\n",
        "\n",
        "They lack:\n",
        "- emotional awareness\n",
        "- mood-adaptive storytelling\n",
        "- immersive audiovisual representation\n",
        "\n",
        "This project addresses the problem of designing an **emotion-driven AI storyteller**\n",
        "where emotional choices influence narrative progression and generated output.\n",
        "\n",
        "### c. Real-World Relevance & Motivation\n",
        "Emotion-aware storytelling is relevant in:\n",
        "- mental wellness and reflection tools\n",
        "- digital art and creative media\n",
        "- interactive education\n",
        "- human-centered AI systems\n",
        "\n",
        "The motivation is to explore how AI systems can respond to **felt experience**\n",
        "rather than only explicit commands.\n"
      ],
      "metadata": {
        "id": "6F-BT-xh6hET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Understanding & Preparation\n",
        "\n",
        "### a. Dataset Source\n",
        "This project does not rely on a conventional public dataset.\n",
        "Instead, it uses:\n",
        "- **synthetic emotional narrative data** (manually designed)\n",
        "- **AI-generated images** via OpenAI\n",
        "- **AI-generated motion** via diffusion models\n",
        "- **AI-generated speech** via neural TTS\n",
        "\n",
        "Such data sources are appropriate for generative AI system projects.\n"
      ],
      "metadata": {
        "id": "uPXk85kv6xMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic emotional story data definition will follow.\n",
        "# This cell is intentionally kept minimal to emphasize system design.\n"
      ],
      "metadata": {
        "id": "vA_Mly7u60i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bâ€“d. Data Preparation\n",
        "- Narrative text is curated to express emotional tone\n",
        "- Emotion labels act as semantic features\n",
        "- No missing values exist due to controlled synthetic design\n",
        "- Noise is minimized through prompt constraints\n"
      ],
      "metadata": {
        "id": "tNxusnE363d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model / System Design\n",
        "\n",
        "### a. AI Techniques Used\n",
        "This project employs a **hybrid AI system**, combining:\n",
        "- symbolic emotion modeling\n",
        "- prompt-engineered image generation (LLM-based)\n",
        "- diffusion-based video synthesis\n",
        "- neural text-to-speech models\n",
        "\n",
        "### b. Architecture / Pipeline\n",
        "Emotional Choice  \n",
        "â†’ Emotion-Conditioned Story Resolution  \n",
        "â†’ Image Generation  \n",
        "â†’ Motion Synthesis  \n",
        "â†’ Voice Narration  \n",
        "â†’ Final Video Composition\n",
        "\n",
        "### c. Justification of Design Choices\n",
        "- Emotion labels provide controllable affective variation\n",
        "- Prompt engineering ensures mood consistency\n",
        "- Diffusion models enable atmospheric visuals\n",
        "- Notebook-based execution ensures reproducibility\n"
      ],
      "metadata": {
        "id": "yjy8gUsy68WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai diffusers transformers accelerate torch torchvision torchaudio\n",
        "!pip install moviepy TTS pillow imageio\n",
        "!pip install --upgrade jax jaxlib flax\n",
        "!pip install --upgrade --force-reinstall diffusers"
      ],
      "metadata": {
        "id": "9rNWuw5E7OMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import openai\n",
        "import imageio\n",
        "from PIL import Image\n",
        "from moviepy.editor import ImageSequenceClip, AudioFileClip, concatenate_videoclips\n",
        "from TTS.api import TTS\n"
      ],
      "metadata": {
        "id": "HnPhIx2v7SAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI API Key\n",
        "openai.api_key = \"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\""
      ],
      "metadata": {
        "id": "R6gVTo6K7Uwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionalScene:\n",
        "    def __init__(self, scene_id, emotion, text, choices):\n",
        "        self.id = scene_id\n",
        "        self.emotion = emotion\n",
        "        self.text = text\n",
        "        self.choices = choices\n",
        "\n",
        "\n",
        "story_graph = {\n",
        "    \"S1\": EmotionalScene(\n",
        "        \"S1\",\n",
        "        emotion=\"longing\",\n",
        "        text=\"Morning arrives quietly. The world feels tender, as if waiting for you to choose yourself.\",\n",
        "        choices={\n",
        "            \"Reach outward\": \"S2\",\n",
        "            \"Stay inward\": \"S3\"\n",
        "        }\n",
        "    ),\n",
        "    \"S2\": EmotionalScene(\n",
        "        \"S2\",\n",
        "        emotion=\"hope\",\n",
        "        text=\"You step forward gently. The air feels lighter, as if it believes in you.\",\n",
        "        choices={\n",
        "            \"Trust the feeling\": \"END\"\n",
        "        }\n",
        "    ),\n",
        "    \"S3\": EmotionalScene(\n",
        "        \"S3\",\n",
        "        emotion=\"melancholy\",\n",
        "        text=\"You remain where you are. Not from fear, but from needing to feel more deeply.\",\n",
        "        choices={\n",
        "            \"Accept the stillness\": \"END\"\n",
        "        }\n",
        "    )\n",
        "}\n"
      ],
      "metadata": {
        "id": "HHYiu6P974yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring emotional narrative data\n",
        "for sid, scene in story_graph.items():\n",
        "    print(\n",
        "        \"Scene:\", sid,\n",
        "        \"| Emotion:\", scene.emotion,\n",
        "        \"| Choices:\", list(scene.choices.keys())\n",
        "    )\n"
      ],
      "metadata": {
        "id": "wINeyCfE793S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_emotional_path(choices):\n",
        "    path = [\"S1\"]\n",
        "    current = \"S1\"\n",
        "\n",
        "    for choice in choices:\n",
        "        next_scene = story_graph[current].choices.get(choice)\n",
        "        if next_scene == \"END\":\n",
        "            break\n",
        "        path.append(next_scene)\n",
        "        current = next_scene\n",
        "\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "27kCEbPZ8BcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_emotional_image(scene, output_path):\n",
        "    prompt = (\n",
        "        f\"Soft cinematic scene, emotional tone: {scene.emotion}, \"\n",
        "        f\"warm lighting, shallow depth of field, poetic atmosphere, \"\n",
        "        f\"no characters facing camera\"\n",
        "    )\n",
        "\n",
        "    response = openai.Image.create(\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "\n",
        "    image_url = response[\"data\"][0][\"url\"]\n",
        "    image = Image.open(imageio.imread(image_url))\n",
        "    image.save(output_path)\n"
      ],
      "metadata": {
        "id": "YZ5181Fu8EFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-1\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "gIpgMwsq8HSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76060f5"
      },
      "source": [
        "### 1. Get your Hugging Face API Token\n",
        "\n",
        "- Go to [Hugging Face settings page](https://huggingface.co/settings/tokens).\n",
        "- Generate a new token with **\"write\"** role.\n",
        "- Copy the token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "291d6099"
      },
      "source": [
        "### 2. Store the token in Colab Secrets\n",
        "\n",
        "- In Colab, click on the **\"ðŸ”‘\" (Secrets)** icon in the left panel.\n",
        "- Add a new secret with the name `HF_TOKEN` and paste your copied Hugging Face API token as the value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0cbcf9"
      },
      "source": [
        "### 3. Log in to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddb2fef3"
      },
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the token from Colab secrets\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "login(token=HF_TOKEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotional_image_to_video(image_path, output_video):\n",
        "    base_image = Image.open(image_path).convert(\"RGB\").resize((512, 512))\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(6):\n",
        "        result = pipe(\n",
        "            prompt=\"slow cinematic motion, emotional softness, subtle light movement\",\n",
        "            image=base_image,\n",
        "            strength=0.5,\n",
        "            guidance_scale=6.5\n",
        "        ).images[0]\n",
        "\n",
        "        frames.append(result)\n",
        "        base_image = result\n",
        "\n",
        "    clip = ImageSequenceClip(frames, fps=5)\n",
        "    clip.write_videofile(output_video, verbose=False, logger=None)\n"
      ],
      "metadata": {
        "id": "-nJ5Pv8x8Jud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tts = TTS(\"tts_models/en/ljspeech/tacotron2-DDC\")\n",
        "\n",
        "def narrate_emotion(text, output_audio):\n",
        "    tts.tts_to_file(text=text, file_path=output_audio)\n"
      ],
      "metadata": {
        "id": "iVAxk9QW8Msr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "choices = [\"Reach outward\", \"Trust the feeling\"]\n",
        "story_path = resolve_emotional_path(choices)\n",
        "\n",
        "video_segments = []\n",
        "\n",
        "for scene_id in story_path:\n",
        "    scene = story_graph[scene_id]\n",
        "\n",
        "    img_path = f\"outputs/{scene_id}.png\"\n",
        "    vid_path = f\"outputs/{scene_id}.mp4\"\n",
        "    aud_path = f\"outputs/{scene_id}.wav\"\n",
        "\n",
        "    generate_emotional_image(scene, img_path)\n",
        "    emotional_image_to_video(img_path, vid_path)\n",
        "    narrate_emotion(scene.text, aud_path)\n",
        "\n",
        "    clip = AudioFileClip(vid_path).set_audio(AudioFileClip(aud_path))\n",
        "    video_segments.append(clip)\n"
      ],
      "metadata": {
        "id": "9ygNhCVR8Pi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_video = concatenate_videoclips(video_segments)\n",
        "final_video.write_videofile(\"outputs/emotional_story.mp4\")\n"
      ],
      "metadata": {
        "id": "MluU_Kag8Shc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video(\"outputs/emotional_story.mp4\", embed=True)\n"
      ],
      "metadata": {
        "id": "6RkZnZEJ8vGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluation & Analysis\n",
        "\n",
        "### a. Metrics Used\n",
        "Due to the creative nature of the project,\n",
        "evaluation is primarily **qualitative**, focusing on:\n",
        "- emotional coherence\n",
        "- narrative consistency\n",
        "- alignment between emotion and generated output\n",
        "\n",
        "### b. Sample Outputs\n",
        "Generated videos for different emotional paths\n",
        "are displayed directly within this notebook.\n",
        "\n",
        "### c. Performance Analysis & Limitations\n",
        "- Video motion coherence is limited by compute\n",
        "- Emotional interpretation is subjective\n",
        "- Generation time increases with pipeline depth\n"
      ],
      "metadata": {
        "id": "t7tLOCWv8z9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Ethical Considerations & Responsible AI\n",
        "\n",
        "- No personal or sensitive data is used\n",
        "- Emotional categories are abstract and non-diagnostic\n",
        "- No impersonation of real individuals\n",
        "- The system avoids emotional manipulation\n"
      ],
      "metadata": {
        "id": "8Q2DLCOz84l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**  \n",
        "Computationally expensive steps (prompt tuning, evaluation) are completed beforehand.  \n",
        "This demo cell performs real-time controlled story generation.\n"
      ],
      "metadata": {
        "id": "VGhJOqcf9DHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained / configured storyteller\n",
        "storyteller = EmotionalStoryteller(load_cached=True)\n",
        "\n",
        "# Generate story with specific emotional conditioning\n",
        "storyteller.generate(\n",
        "    emotion=\"longing\",\n",
        "    culture=\"Indian\",\n",
        "    tone=\"soft and poetic\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "9ZCjOuBT9sEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storyteller.generate(\n",
        "    emotion=\"resilience\",\n",
        "    culture=\"Indian\",\n",
        "    tone=\"grounded and hopeful\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "u2A31Y8V9yB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion & Future Scope\n",
        "\n",
        "### Summary\n",
        "This project demonstrates a fully notebook-executed,\n",
        "emotion-aware AI storytelling system that generates\n",
        "multimedia narratives based on emotional input.\n",
        "\n",
        "### Future Scope\n",
        "- Emotion-conditioned voice modulation\n",
        "- Improved temporal coherence in video synthesis\n",
        "- Personalized emotional modeling\n",
        "- Interactive user interfaces\n"
      ],
      "metadata": {
        "id": "z_IEO6Dk87sw"
      }
    }
  ]
}